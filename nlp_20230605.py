# -*- coding: utf-8 -*-
"""NLP_20230605.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r8ptXAveWiHjfFueSQ3YBY-LqqcGAOpI
"""

# import libraries

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from collections import deque, Counter
import os
import zipfile
import urllib.request
import scipy as sc
import time
import pandas as pd
from tqdm import tqdm

# hyperparameters

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vocab_size = 50000
batch_size = 128
num_of_iters = 15000

# original skip-gram

class SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGram, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, center_word):
        center_embeds = self.embeddings(center_word)
        center_out = self.linear(center_embeds)
        return center_out

# direction-aware skip-gram

class D_SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(D_SkipGram, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.w_prev = nn.Linear(embedding_dim, vocab_size)
        self.w_next = nn.Linear(embedding_dim, vocab_size)

    def forward(self, center_word, prev):
        center_embeds = self.embeddings(center_word)
        center_out = self.w_prev(center_embeds) if prev else self.w_next(center_embeds)
        return center_out

# download and uncompress the data

def download_data():
    print('Processing data...')

    filename = 'wiki-0.1percent.txt'
    filepath = './' + filename

    # Read the file
    with open(filepath, 'r') as f:
        words = f.read().lower().split()

    return words

# build the dataset and vocabulary

def build_dataset(words, vocab_size):
    print('build_dataset')

    count = [['UNK', -1]]
    count.extend(Counter(words).most_common(vocab_size - 1))
    word2idx = {word: i for i, (word, _) in enumerate(count)}

    data = []
    unk_count = 0
    for word in words:
        index = word2idx.get(word, 0)  # 0 is the index of 'UNK'
        if index == 0:
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count

    idx2word = {i: word for word, i in word2idx.items()}

    return data, word2idx, idx2word

# generate batch for training

def generate_batch(data, window_size):
    print('generate_batch')

    skip_grams = []

    for i in range(window_size, len(data) - window_size):
        target = data[i]
        context = data[i-window_size:i] + data[i+1:i+1+window_size]
        skip_grams.append((target, context))

    return skip_grams

# train the model

def train_model(data, window_size, embedding_dim, epoch, is_directional):
    print('train_model')
    start_time = time.time()

    batch = generate_batch(data, window_size)

    model = D_SkipGram(vocab_size, embedding_dim) if is_directional else SkipGram(vocab_size, embedding_dim)
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters())

    for epoch in range(epoch):
        total_loss = 0

        for i in tqdm(range(num_of_iters)):
            optimizer.zero_grad()

            _input = [batch[i + num_of_iters * j][0] for j in range(batch_size)]
            input = torch.tensor(_input, device=device)
            _context_words = [batch[i + num_of_iters * j][1] for j in range(batch_size)]
            context_words = torch.tensor(_context_words, device=device)

            losses = torch.zeros(batch_size, window_size * 2, device=device)

            output = model(input, True) if is_directional else model(input)
            for k in range(window_size):
                context_word = context_words[:, k]
                losses[:, k] = criterion(output, context_word)
            output = model(input, False) if is_directional else model(input)
            for k in range(window_size, window_size*2):
                context_word = context_words[:, k]
                losses[:, k] = criterion(output, context_word)

            loss = torch.sum(losses)
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

        print(f"Epoch: {epoch+1}, Loss: {total_loss/num_of_iters}")

    end_time = time.time()

    return model, end_time - start_time

# record start/end time

def save_embedding(embedding, file_prefix):
    np.save(file_prefix + '.npy', embedding)
    np.savetxt(file_prefix + '.csv', embedding, delimiter=',')

# plot the result

def plot_result(embedding, word2idx, idx2word, file_prefix):
    print('plot_result')

    plot_only = 500
    low_dim_embs = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(embedding[:plot_only, :])
    labels = [idx2word[i] for i in np.arange(plot_only)]

    plt.figure(figsize=(18, 18))  # in inches

    for i, label in enumerate(labels):
        x, y = low_dim_embs[i, :]
        plt.scatter(x, y)
        plt.annotate(label,
                    xy=(x, y),
                    xytext=(5, 2),
                    textcoords='offset points',
                    ha='right',
                    va='bottom',
                    size=10)  # 글씨 크기 조정

    plt.savefig(file_prefix + '_plot.png')
    plt.close()

# find closest words for each target word

def find_closest(embedding, word2idx, idx2word, file_prefix):
    print('find_closest')

    words = ['can', 'good', 'number', 'january', 'word', 'first', 'because', 'special', 'computer', 'result']
    range_to_find = 10

    f = open(file_prefix + '_closest.txt', 'w')

    for word in words:
        index = word2idx[word]
        vectors = embedding
        query = vectors[index]

        cos_sim = np.dot(vectors, query) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query))
        indices = np.argsort(cos_sim)[::-1][:range_to_find]
        close_words = [idx2word[i] for i in indices]

        f.write(word + ': ')
        for w in close_words:
            f.write(w + ' ')
        f.write('\n')

    f.close()

# read DBs for evaluation

def readDB():
    print('readDB')

    menDB, simlexDB = {}, {}

    with open('men.txt', 'r', encoding='utf-8') as f:
        f.readline() # header
        data = f.readline()
        while data:
            w1, w2, score = data.split()
            menDB[(w1, w2)] = float(score)
            data = f.readline()

    with open('simlex-999.txt', 'r', encoding='utf-8') as f:
        f.readline() # header
        data = f.readline()
        while data:
            w1, w2, score = data.split()
            simlexDB[(w1, w2)] = float(score)
            data = f.readline()

    return menDB, simlexDB

# get evaluation scores

def eval_model(embedding, word2idx, idx2word, file_prefix):
    print('eval_model')

    menDB, simlexDB = readDB()

    f = open(file_prefix + '_eval.txt', 'w')

    for db, dbName in [(menDB, 'MEN'), (simlexDB, 'SimLex-999')]:
        calc, man = [], []
        for w1, w2 in db:
            if w1 in word2idx and w2 in word2idx:
                wv1, wv2 = embedding[word2idx[w1]], embedding[word2idx[w2]]
                cos_sim = np.dot(wv1, wv2) / (np.linalg.norm(wv1) * np.linalg.norm(wv2))
            else:
                cos_sim = 0.0
            calc.append(cos_sim)
            man.append(db[(w1, w2)])
        coef, _ = sc.stats.spearmanr(calc, man)
        f.write(dbName + ' : ' + str(coef) + '\n')

    f.close()

# test the model

def test_model(model, word2idx, idx2word, file_prefix):
    print('test_model')

    embedding = model.embeddings.weight.data.cpu().numpy()

    save_embedding(embedding, file_prefix)
    plot_result(embedding, word2idx, idx2word, file_prefix)
    find_closest(embedding, word2idx, idx2word, file_prefix)
    eval_model(embedding, word2idx, idx2word, file_prefix)

# record start/end time

def record_time(file_prefix, elapsed_time):
    with open(file_prefix + '_log.txt', 'w') as f:
        f.write('elapsed_time: ' + str(elapsed_time))

# do experiment

def experiment(data, word2idx, idx2word, window_size, embedding_dim, epoch, is_directional):
    print('experiment')

    file_prefix = 'w=' + str(window_size) + '_emb=' + str(embedding_dim) + '_ep=' + str(epoch) + '_d=' + str(is_directional)

    model, elapsed_time = train_model(data, window_size, embedding_dim, epoch, is_directional)
    record_time(file_prefix, elapsed_time)

    test_model(model, word2idx, idx2word, file_prefix)

words = download_data()
data, word2idx, idx2word = build_dataset(words, vocab_size)

# Minsik
experiment(data, word2idx, idx2word, window_size=2, embedding_dim=64, epoch=10, is_directional=False)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=64, epoch=5, is_directional=False)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=64, epoch=20, is_directional=False)
# experiment(data, word2idx, idx2word, window_size=1, embedding_dim=64, epoch=10, is_directional=False)
# experiment(data, word2idx, idx2word, window_size=3, embedding_dim=64, epoch=10, is_directional=False)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=32, epoch=10, is_directional=False)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=128, epoch=10, is_directional=False)

# Changho
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=64, epoch=20, is_directional=True)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=64, epoch=10, is_directional=True)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=64, epoch=40, is_directional=True)
# experiment(data, word2idx, idx2word, window_size=1, embedding_dim=64, epoch=20, is_directional=True)
# experiment(data, word2idx, idx2word, window_size=3, embedding_dim=64, epoch=20, is_directional=True)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=32, epoch=20, is_directional=True)
# experiment(data, word2idx, idx2word, window_size=2, embedding_dim=128, epoch=20, is_directional=True)